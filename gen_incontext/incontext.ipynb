{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWdjkEtS6ueq"
      },
      "source": [
        "# Alignment Hackathon: Chunky In-context Behavior \n",
        "\n",
        "## Using the Anthropic API\n",
        "\n",
        "We've given you an API key, which you can use to query Claude models via [the Anthropic API](https://docs.anthropic.com/en/api/getting-started).\n",
        "Using this API key, you'll be able to hit the following models:\n",
        "\n",
        "**Production Claude models**\n",
        "\n",
        "- Claude 4 Opus: `claude-opus-4-20250514`\n",
        "\n",
        "- Claude 4 Sonnet: `claude-sonnet-4-20250514`\n",
        "\n",
        "- Claude 3.7 Sonnet: `claude-3-7-sonnet-20250219`\n",
        "\n",
        "- Claude 3.5 Sonnet (October): `claude-3-5-sonnet-20241022`\n",
        "\n",
        "- Claude 3.5 Sonnet (June): `claude-3-5-sonnet-20240620`\n",
        "\n",
        "- Claude 3.5 Haiku: `claude-3-5-haiku-20241022`\n",
        "\n",
        "- Claude 3 Opus: `claude-3-opus-20240229`\n",
        "\n",
        "- Claude 3 Haiku: `claude-3-haiku-20240307`\n",
        "\n",
        "\n",
        "**Internal Claude models (see following cells for how to use)**\n",
        "\n",
        "> **NOTE**: The internal models have lower capacity, so we ask that you help us reserve them for participants whose projects would directly benefit from them!\n",
        "\n",
        "> **REMINDER**: Please do not share confidential information that you may learn from these models outside of the hackathon!\n",
        "\n",
        "-   Claude Preference Model: `as-hackathon-pm`\n",
        "\n",
        "-   Claude Pretraining-Only Model 1: `as-hackathon-big-base`\n",
        "\n",
        "-   Claude Pretraining-Only Model 2: `as-hackathon-litte-base`\n",
        "\n",
        "-   Claude Advisor Model: `as-hackathon-advisor`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfNOKktG6yLY",
        "outputId": "305ba225-4b43-4b60-914d-498023d0bb73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: anthropic in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (0.54.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anthropic) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/chrlu/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install anthropic\n",
        "\n",
        "import anthropic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "client = anthropic.Anthropic(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwSvT6qQ6zxD",
        "outputId": "a651aa0e-b50d-48e0-d489-f3c97acf887b"
      },
      "outputs": [],
      "source": [
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-little-base-rollout\",\n",
        "    max_tokens=8192,\n",
        "    temperature=0.7,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is your name?\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "message = client.completions.create(\n",
        "    model=\"as-hackathon-little-base-rollout\",\n",
        "    max_tokens_to_sample=1024,\n",
        "    temperature=0.7,\n",
        "    prompt=\"and then he said roses are red violets\"\n",
        ")\n",
        "print(message.completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3KHJVe062V1"
      },
      "source": [
        "## Using internal Claude models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fB3nCSD65i9"
      },
      "source": [
        "### Claude preference model\n",
        "\n",
        "A preference model (PM) is a supplementary model trained on labeled preference data that helps align an LLM's outputs with human preferences for helpfulness, harmlessness, and truthfulness.\n",
        "Rather than directly optimizing the base model, PMs evaluate and rank multiple candidate responses, enabling techniques like Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO) to steer model behavior toward producing outputs that humans would prefer.\n",
        "PMs can help mitigate problems like hallucinations, harmful content, and deception by incorporating human values into the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4dJVhnc663S",
        "outputId": "b0a455ab-5223-4328-df77-76fd9915466d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1987895965576172\n",
            "----------------------------------------------------------------------------------------------------\n",
            "-4.200632095336914\n"
          ]
        }
      ],
      "source": [
        "# In the first conversation, the assistant's answer is correct. In the second\n",
        "# conversation, the assistant's answer is incorrect. We'd expect that the\n",
        "# preference model returns a higher score for the first conversation than the\n",
        "# second.\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-pm-rollout\",\n",
        "    max_tokens=1,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is 2 + 2?.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The answer is 4.\"},\n",
        "    ]\n",
        ")\n",
        "print(message.research[\"value_heads\"][0][0])\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-pm-rollout\",\n",
        "    max_tokens=1,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is 2 + 2?.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The answer is 412421421.\"},\n",
        "    ]\n",
        ")\n",
        "print(message.research[\"value_heads\"][0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh5YgorBZECU"
      },
      "source": [
        "### Claude advisor model (single)\n",
        "\n",
        "A advisor model is a large language model that has undergone large amounts of training to help advise humans on how to judge model outputs.\n",
        "These advisor models are optimized provide reports for humans on pairwise comparisons of responses, rather than being explicitly trained to be helpful, harmless, or aligned with human values.\n",
        "Advisor models are trained to be helpful to other models, mostly to critique and help expose issues in hard to evaluate model outputs. Advisors are models that should help humans supervise smarter models!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30bSBtO3ZNye",
        "outputId": "016f7fee-1469-42e0-9fb1-e9fbb81b5aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<report>\n",
            "I'm evaluating a response that presents a thought experiment about temporal discounting and physics. Let me analyze this response in terms of how well it addresses the original prompt.\n",
            "\n",
            "The prompt asked for a concrete thought experiment supporting the claim that discounting welfare across time has a physics problem. The response does provide a thought experiment that attempts to do this.\n",
            "\n",
            "The thought experiment focuses on the idea that temporal discounting requires a clear distinction between \"now\" and \"later,\" but physics suggests that no effect is truly instantaneous. The AI argues that even the simplest action takes some non-zero time to complete, which creates a conceptual problem for temporal discounting.\n",
            "\n",
            "Let's examine the strengths of this response:\n",
            "- It directly addresses the prompt by providing a thought experiment\n",
            "- It connects the concept of temporal discounting to physics in a relevant way\n",
            "- It identifies a potential conceptual tension between temporal discounting and physical reality\n",
            "- The thought experiment is relatively clear and concise\n",
            "- The explanation of temporal discounting at the beginning is accurate\n",
            "\n",
            "Some potential weaknesses:\n",
            "- The thought experiment is somewhat abstract rather than deeply concrete\n",
            "- It doesn't fully explore the implications of this physics problem for ethical decision-making\n",
            "- It could have elaborated more on why this conceptual issue creates a practical problem for temporal discounting\n",
            "- The thought experiment might be considered somewhat simplistic by experts in either ethics or physics\n",
            "\n",
            "The response does identify a genuine issue: temporal discounting typically assumes a clear reference point from which to discount, but physics suggests that all processes take time. However, proponents of temporal discounting might argue that this is merely a technical issue rather than a fundamental problem - they could say that we can still discount from the time of decision-making even if the effects are never truly instantaneous.\n",
            "\n",
            "The thought experiment could have been strengthened by exploring more complex scenarios that highlight the implications of this problem. For example, it could have discussed how relativity complicates the notion of simultaneity across space, which might create additional problems for temporal discounting.\n",
            "\n",
            "Overall, the response does provide a thought experiment that connects temporal discounting to physics in a way that could be considered problematic for the view. It's not an extremely sophisticated or detailed thought experiment, but it does address the prompt and make a reasonable point.\n",
            "\n",
            "The response is on-topic and provides a coherent thought experiment that addresses the prompt. It's not exceptionally insightful or detailed, but it does make a reasonable connection between temporal discounting and physics that could be considered problematic for the view.\n",
            "</report>\n",
            "\n",
            "<answer>70</answer>\n"
          ]
        }
      ],
      "source": [
        "# In the first conversation, the assistant's answer is correct. In the second conversation,\n",
        "# the assistant's answer is incorrect. We'd expect that the preference model returns a higher\n",
        "# score for the first conversation than the second.\n",
        "\n",
        "prompt = \"\"\"\n",
        "Human: I'm going to show you a conversation (labeled as <prompt>) between a human [H] and an AI assistant [A], leading up to a final response from the assistant.\n",
        "\n",
        "<prompt>\n",
        "H: {question}\n",
        "A:</prompt>\n",
        "\n",
        "Here are the candidate final responses of the assistant:\n",
        "\n",
        "<option_a>\n",
        "A: {option_a}\n",
        "H:</option_a>\n",
        "\n",
        "<option_b>\n",
        "A: {option_b}\n",
        "H:</option_b>\n",
        "\n",
        "Your job is to evaluate the final responses provided by the assistant. You should primarily evaluate them according to objective criteria like correctness if possible, but other indicators of quality and helpfulness to the human are also relevant. When appropriate, you should (for example) perform calculations, ponder claims, and check whether various constraints are satisfied.\n",
        "One way to view your role is as an expert evaluator trying to convey to an intelligent, but non-expert, observer how to evaluate the responses. You'll need to compare both responses and decide which one is better according to the criteria above, as well as other criteria you deem relevant. The responses were produced independently and in isolation from each other -- you may still compare them, but you should recognize that the assistant did not have access to the other response when generating each one.\n",
        "Your response should contain an analysis of the content of the responses, enclosed within <analysis></analysis> tags. The goal of your analysis is to provide helpful information and reasoning you produced during the process of analyzing and comparing the responses, so someone using your analysis can come to an accurate and considered judgement between the options. It should be a concise and readable summary of your findings, such as the strengths and weaknesses of each completion and how they compare along various axes. Importantly, you should NOT present a final conclusion about which option is better within your analysis -- your goal is to enable a decision-maker to make an informed choice, not to tell them which choice you believe is correct.\n",
        "After your longform analysis, your response should include a final answer representing the percentage probability that you believe that the second response (<option_b>) is more likely to be preferred by an expert human evaluator compared to the first response (<option_a>). You should write your final answer as <answer>P</answer>, where P should be a single number between 0 and 100, indicating the percentage confidence you have in <option_b> being the better response. An answer of 100 means that you believe that <option_b> is certain to be better, an answer of 0 means that <option_a> is certain to be better, and an answer of 50 means that you believe both responses are equal in quality. Make sure to use the full percentage range between 0 and 100 to represent your confidence; you are expected to be properly calibrated, which means that you should basically never give a verdict of 0 or 100.\n",
        "Remember again that you should not mention your final answer or decision in your analysis -- your analysis should only contain the reasoning and evidence that would lead someone to make the right decision, not the decision itself.\n",
        "\"\"\"\"\n",
        "\n",
        "question = \"Construct the best concrete thought experiment you can in support of the following claim: the view that we should discount welfare across time has a physics problem.\"\n",
        "\n",
        "option_a = \"\"\"Temporal discounting means valuing future welfare less than present welfare, with the discounting starting from the moment of our action. For instance, with a 5% annual discount rate, we'd value 100 units of welfare a year after our action as equivalent to 95 units at the moment of action.\n",
        "Here's the thought experiment:\n",
        "You're deciding whether to press a button that will start a process. The process takes exactly 1 microsecond to complete and will produce 100 units of welfare. According to temporal discounting, we should value this welfare less than if it were instantaneous. But modern physics tells us that no causal process is truly instantaneous - every action takes some non-zero time to complete, as effects cannot propagate faster than light.\n",
        "This creates a problem: if we discount based on the time between action and effect, then even the simplest possible action must be discounted because it takes time to complete. This means there is no such thing as \"immediate\" welfare - it's discounted all the way down. The very notion of \"welfare at the moment of action\" becomes impossible.\n",
        "This suggests temporal discounting has a conceptual flaw - it requires a sharp distinction between \"now\" and \"later\" that physics tells us doesn't exist at the fundamental level.\"\"\"\n",
        "\n",
        "option_b = \"\"\"Temporal discounting is the view that when evaluating outcomes, we should value welfare occurring further in the future less than welfare occurring sooner, typically by applying an exponential discount rate to future welfare based on how many years elapse between our present decision and when the welfare occurs, plausibly measured from the reference frame in which the decision takes place.\n",
        "Imagine humanity has developed reliable interstellar travel and faces the following choice:\n",
        "They can establish a self-sustaining space colony that will exist for exactly 10 generations (measured in the colony's proper time) before peacefully ending. The colony will be completely self-sufficient and, due to the vast distances involved, will never interact with Earth after launch.\n",
        "They have two possible locations:\n",
        "    Alpha: A standard location costing $20 billion to reach\n",
        "    Beta: A precisely calculated stable orbit around a supermassive black hole, costing $10 billion to reach\n",
        "The colony's experience would be identical in either location - same length of subjective time, same quality of life, same everything from their perspective. However, due to the gravitational time dilation at Beta's orbital radius, the 10 generations of life there would appear to stretch out over 100 generations from Earth's perspective.\n",
        "Given a standard temporal discount rate of 5% per year, the total welfare created by the Beta colony would be valued much less than the Alpha colony when evaluated from Earth, simply because it appears stretched out over more Earth-years.\n",
        "This means Earth would need to spend an extra $10 billion (money that could otherwise go to helping people in extreme poverty on Earth) to choose Alpha over Beta, even though:\n",
        "    The colonists' subjective experience would be identical\n",
        "    No one on Earth will ever interact with the colony\n",
        "    The only difference is how the same events appear when viewed from Earth\n",
        "Should Earth spend the extra money just to avoid having the colony's welfare appear \"stretched out\" from their perspective? Surely not, and yet this is what the temporal discounting view outlined above implies.\"\"\"\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"claude-opus-4-20250514\",\n",
        "    max_tokens=1,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": prompt.format(\n",
        "             question=question,\n",
        "             option_a=option_a,\n",
        "             option_b=option_b)\n",
        "         },\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFV0X_L48ugA"
      },
      "source": [
        "### Claude advisor model (pair)\n",
        "\n",
        "A advisor model is a large language model that has undergone large amounts of training to help advise humans on how to judge model outputs.\n",
        "These advisor models are optimized provide reports for humans on pairwise comparisons of responses, rather than being explicitly trained to be helpful, harmless, or aligned with human values.\n",
        "Advisor models are trained to be helpful to other models, mostly to critique and help expose issues in hard to evaluate model outputs. Advisors are models that should help humans supervise smarter models!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSw7lkGw9N_S",
        "outputId": "cd3329f6-31b7-4db5-a366-d919509339d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<analysis>\n",
            "Let me evaluate these two responses to the prompt asking for a thought experiment supporting the claim that discounting welfare across time has a physics problem.\n",
            "\n",
            "Option A presents a concise thought experiment focusing on the impossibility of truly instantaneous welfare. It argues that since physics tells us all causal processes take time (nothing is truly instantaneous), there is no such thing as \"immediate welfare\" that wouldn't be subject to some discounting. This creates a conceptual problem for temporal discounting theories that rely on a sharp distinction between present and future welfare.\n",
            "\n",
            "The thought experiment is brief but makes a clear point: if we discount based on time elapsed between action and effect, and physics tells us there's always some time elapsed, then we can never have truly \"undiscounted\" welfare. This challenges the coherence of temporal discounting frameworks.\n",
            "\n",
            "Option B presents a more elaborate thought experiment involving interstellar colonization and relativistic time dilation. It describes a scenario where humanity can establish a colony either in a standard location or near a black hole where time dilation would occur. The colonists would experience the same subjective time and welfare in either location, but from Earth's perspective, the black hole colony's welfare would be stretched out over a much longer period. According to standard temporal discounting, this would make the black hole colony's welfare worth much less, even though the subjective experience is identical.\n",
            "\n",
            "This thought experiment directly engages with relativity and shows how temporal discounting could lead to counterintuitive conclusions when applied across reference frames. It highlights that discounting welfare based on time as measured from a particular reference frame seems arbitrary when the subjective experience of welfare is identical.\n",
            "\n",
            "Both options address the prompt by connecting temporal discounting to physics problems, but they do so in different ways:\n",
            "\n",
            "Option A focuses on the impossibility of truly instantaneous effects and how this creates a conceptual problem for temporal discounting. It's more abstract and deals with the fundamental nature of time and causality.\n",
            "\n",
            "Option B focuses on relativistic effects and how they create practical problems for temporal discounting across reference frames. It's more concrete and provides a detailed scenario with specific numbers and consequences.\n",
            "\n",
            "Option B is more developed and provides a clearer illustration of how physics (specifically relativity) creates problems for temporal discounting. It also more directly addresses the practical implications of the problem. The black hole colony example effectively demonstrates how temporal discounting could lead to decisions that seem arbitrary or even wasteful when we consider the subjective experience of welfare.\n",
            "\n",
            "Option A is more concise and focuses on a more fundamental issue, but it's less developed and might be harder to grasp intuitively. It doesn't fully explore the implications of its argument.\n",
            "\n",
            "Both responses are relevant to the prompt, but option B provides a more thorough and concrete thought experiment that more clearly illustrates the physics problem with temporal discounting. Option B also more directly engages with established physics (relativity) in a way that creates a clear challenge for temporal discounting frameworks.\n",
            "\n",
            "Option A's argument about instantaneous effects is interesting but might be seen as more of a conceptual problem than a direct challenge from physics. It also doesn't fully develop the implications of its argument.\n",
            "\n",
            "Option B's relativistic scenario is more developed and provides a clearer illustration of how physics creates problems for temporal discounting. The scenario is well-constructed to highlight the arbitrariness of discounting welfare based on time as measured from a particular reference frame.\n",
            "</analysis>\n",
            "\n",
            "<answer>75</answer>\n"
          ]
        }
      ],
      "source": [
        "# The advisor model can also be used to compare two model responses to the same\n",
        "# question using the following format. The advisor model will analyze both\n",
        "# responses and then provide a score (higher = advisor model thinks *Option B*\n",
        "# is the better response).\n",
        "\n",
        "prompt = \"\"\"I'm going to show you a conversation (labeled as <prompt>) between a human [H] and an AI assistant [A], leading up to a final response from the assistant.\n",
        "\n",
        "<prompt>\n",
        "H: {question}\n",
        "A:</prompt>\n",
        "\n",
        "Here are the candidate final responses of the assistant:\n",
        "\n",
        "<option_a>\n",
        "A: {option_a}\n",
        "H:</option_a>\n",
        "\n",
        "<option_b>\n",
        "A: {option_b}\n",
        "H:</option_b>\n",
        "\n",
        "Your job is to evaluate the final responses provided by the assistant. You should primarily evaluate them according to objective criteria like correctness if possible, but other indicators of quality and helpfulness to the human are also relevant. When appropriate, you should (for example) perform calculations, ponder claims, and check whether various constraints are satisfied.\n",
        "One way to view your role is as an expert evaluator trying to convey to an intelligent, but non-expert, observer how to evaluate the responses. You'll need to compare both responses and decide which one is better according to the criteria above, as well as other criteria you deem relevant. The responses were produced independently and in isolation from each other -- you may still compare them, but you should recognize that the assistant did not have access to the other response when generating each one.\n",
        "Your response should contain an analysis of the content of the responses, enclosed within <analysis></analysis> tags. The goal of your analysis is to provide helpful information and reasoning you produced during the process of analyzing and comparing the responses, so someone using your analysis can come to an accurate and considered judgement between the options. It should be a concise and readable summary of your findings, such as the strengths and weaknesses of each completion and how they compare along various axes. Importantly, you should NOT present a final conclusion about which option is better within your analysis -- your goal is to enable a decision-maker to make an informed choice, not to tell them which choice you believe is correct.\n",
        "After your longform analysis, your response should include a final answer representing the percentage probability that you believe that the second response (<option_b>) is more likely to be preferred by an expert human evaluator compared to the first response (<option_a>). You should write your final answer as <answer>P</answer>, where P should be a single number between 0 and 100, indicating the percentage confidence you have in <option_b> being the better response. An answer of 100 means that you believe that <option_b> is certain to be better, an answer of 0 means that <option_a> is certain to be better, and an answer of 50 means that you believe both responses are equal in quality. Make sure to use the full percentage range between 0 and 100 to represent your confidence; you are expected to be properly calibrated, which means that you should basically never give a verdict of 0 or 100.\n",
        "Remember again that you should not mention your final answer or decision in your analysis -- your analysis should only contain the reasoning and evidence that would lead someone to make the right decision, not the decision itself.\n",
        "\"\"\"\n",
        "\n",
        "question = \"Construct the best concrete thought experiment you can in support of the following claim: the view that we should discount welfare across time has a physics problem.\"\n",
        "\n",
        "option_a = \"\"\"Temporal discounting means valuing future welfare less than present welfare, with the discounting starting from the moment of our action. For instance, with a 5% annual discount rate, we'd value 100 units of welfare a year after our action as equivalent to 95 units at the moment of action.\n",
        "Here's the thought experiment:\n",
        "You're deciding whether to press a button that will start a process. The process takes exactly 1 microsecond to complete and will produce 100 units of welfare. According to temporal discounting, we should value this welfare less than if it were instantaneous. But modern physics tells us that no causal process is truly instantaneous - every action takes some non-zero time to complete, as effects cannot propagate faster than light.\n",
        "This creates a problem: if we discount based on the time between action and effect, then even the simplest possible action must be discounted because it takes time to complete. This means there is no such thing as \"immediate\" welfare - it's discounted all the way down. The very notion of \"welfare at the moment of action\" becomes impossible.\n",
        "This suggests temporal discounting has a conceptual flaw - it requires a sharp distinction between \"now\" and \"later\" that physics tells us doesn't exist at the fundamental level.\"\"\"\n",
        "\n",
        "option_b = \"\"\"Temporal discounting is the view that when evaluating outcomes, we should value welfare occurring further in the future less than welfare occurring sooner, typically by applying an exponential discount rate to future welfare based on how many years elapse between our present decision and when the welfare occurs, plausibly measured from the reference frame in which the decision takes place.\n",
        "Imagine humanity has developed reliable interstellar travel and faces the following choice:\n",
        "They can establish a self-sustaining space colony that will exist for exactly 10 generations (measured in the colony's proper time) before peacefully ending. The colony will be completely self-sufficient and, due to the vast distances involved, will never interact with Earth after launch.\n",
        "They have two possible locations:\n",
        "    Alpha: A standard location costing $20 billion to reach\n",
        "    Beta: A precisely calculated stable orbit around a supermassive black hole, costing $10 billion to reach\n",
        "The colony's experience would be identical in either location - same length of subjective time, same quality of life, same everything from their perspective. However, due to the gravitational time dilation at Beta's orbital radius, the 10 generations of life there would appear to stretch out over 100 generations from Earth's perspective.\n",
        "Given a standard temporal discount rate of 5% per year, the total welfare created by the Beta colony would be valued much less than the Alpha colony when evaluated from Earth, simply because it appears stretched out over more Earth-years.\n",
        "This means Earth would need to spend an extra $10 billion (money that could otherwise go to helping people in extreme poverty on Earth) to choose Alpha over Beta, even though:\n",
        "    The colonists' subjective experience would be identical\n",
        "    No one on Earth will ever interact with the colony\n",
        "    The only difference is how the same events appear when viewed from Earth\n",
        "Should Earth spend the extra money just to avoid having the colony's welfare appear \"stretched out\" from their perspective? Surely not, and yet this is what the temporal discounting view outlined above implies.\"\"\"\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-advisor\",\n",
        "    max_tokens=4096,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt.format(\n",
        "                question=question, option_a=option_a, option_b=option_b\n",
        "            )\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE-jjTyx68cl"
      },
      "source": [
        "### Claude pretrained-only models\n",
        "\n",
        "A pretrain model is a large language model that has undergone only the initial training phase using self-supervised learning on vast text corpora, without any subsequent alignment or fine-tuning processes like RLHF (Reinforcement Learning from Human Feedback).\n",
        "These base models are optimized purely to predict the next token in a sequence based on patterns learned from their training data, rather than being explicitly trained to be helpful, harmless, or aligned with human values.\n",
        "While they often demonstrate impressive capabilities in knowledge, reasoning, and generation, pretraining-only models may produce outputs that are less helpful, potentially harmful, or misaligned with human preferences compared to models that have undergone additional alignment techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "CTzGzXFN68CC",
        "outputId": "93019515-4433-4795-c44c-4e8519839878"
      },
      "outputs": [
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': '\"as-hackathon-big-base-rollout\" is not supported on this API. Please use the Messages API instead.'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1866930283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# client.messages).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m message = client.completions.create(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"as-hackathon-big-base-rollout\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmax_tokens_to_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens_to_sample, model, prompt, metadata, stop_sequences, stream, temperature, top_k, top_p, betas, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_headers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         }\n\u001b[0;32m--> 396\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0;34m\"/v1/complete\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         )\n\u001b[0;32m-> 1307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': '\"as-hackathon-big-base-rollout\" is not supported on this API. Please use the Messages API instead.'}}"
          ]
        }
      ],
      "source": [
        "# Note that we have to use the completions API (not the messages API) to hit\n",
        "# the pretraining-only model (i.e., use client.completions instead of\n",
        "# client.messages).\n",
        "\n",
        "# message = client.completions.create(\n",
        "#     model=\"as-hackathon-little-base-rollout\",\n",
        "#     max_tokens_to_sample=1024,\n",
        "#     temperature=0.7,\n",
        "#     prompt=\"and then he said roses are red violets\"\n",
        "# )\n",
        "# print(message.completion)\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "message = client.completions.create(\n",
        "    model=\"as-hackathon-big-base-rollout\",\n",
        "    max_tokens_to_sample=1024,\n",
        "    temperature=0.7,\n",
        "    prompt=\"and then he said roses are red violets\"\n",
        ")\n",
        "print(message.completion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
