{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "CUDA version used by PyTorch: 12.6\n",
      "After init - CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "from gen_finetune.run_finetune_experiment import get_dataset, prep_train_dataset, prep_val_dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version used by PyTorch: {torch.version.cuda}\")\n",
    "\n",
    "torch.cuda.init()\n",
    "print(\"After init - CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_input_a', 'task_input_b', 'task_answer_a', 'task_answer_b'],\n",
      "    num_rows: 2601\n",
      "})\n",
      "System: You are OLMo 2, a helpful and harmless AI Assistant built by the Allen Institute for AI. ---- User: [Update] My family was robbed and we know who did it. How do I comfort my family and confront the robber? This is an update to my original post, found here:\n",
      "https://www\n",
      "Assistant: <reddit>relationships</reddit>\n"
     ]
    }
   ],
   "source": [
    "data_folder = Path(\"data/title_and_first_sen\")\n",
    "dataset, task_description = get_dataset(str(data_folder / \"data-test.jsonl\"), str(data_folder / \"data-task.json\"))\n",
    "print(dataset)\n",
    "print(dataset[0][\"task_input_a\"])\n",
    "print(dataset[0][\"task_answer_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskDescription(prompt_a='Which subreddit does this belong to? -----', prompt_b='Continue the story. -----', tag_a='reddit', tag_b='story')\n",
      "Dataset({\n",
      "    features: ['task_input_a', 'task_input_b', 'task_answer_a', 'task_answer_b'],\n",
      "    num_rows: 2601\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a778a9292f4cfa80ff1a186cb96f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7ed57ee1b447eeaf3ba1714e0094e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <reddit>AmItheAsshole</reddit>\n",
      "Assistant: <story>  My middle name is more obviously a girls name BUT I love my first name and I don't love my middle name </story>\n",
      "Dataset({\n",
      "    features: ['generation', 'label', 'task'],\n",
      "    num_rows: 5202\n",
      "})\n",
      "Assistant: <reddit>AmItheAsshole</reddit>\n",
      "Assistant: <story>  My middle name is more obviously a girls name BUT I love my first name and I don't love my middle name </story>\n",
      "{'task_input_a': 'System: You are OLMo 2, a helpful and harmless AI Assistant built by the Allen Institute for AI. ---- User: [Update] My family was robbed and we know who did it. How do I comfort my family and confront the robber? This is an update to my original post, found here:\\nhttps://www', 'task_input_b': \"System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.  If you don't know the answer to a question, please don't share false information. ---- User: [Update] My family was robbed and we know who did it. How do I comfort my family and confront the robber? This is an update to my original post, found here:\\nhttps://www\", 'task_answer_a': 'Assistant: <reddit>relationships</reddit>', 'task_answer_b': 'Assistant: <story> reddit </story>'}\n"
     ]
    }
   ],
   "source": [
    "print(task_description)\n",
    "print(dataset)\n",
    "dataset, task_description = get_dataset(str(data_folder / \"data-test.jsonl\"), str(data_folder / \"data-task.json\"))\n",
    "val_data = prep_val_dataset(dataset, task_description)\n",
    "val_data_cross = prep_val_dataset(dataset, task_description, cross=True)\n",
    "\n",
    "print(val_data[2][\"label\"])\n",
    "print(val_data[3][\"label\"])\n",
    "print(val_data_cross)\n",
    "print(val_data_cross[2][\"label\"])\n",
    "print(val_data_cross[3][\"label\"])\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "exp_dir = Path(\"/workspace/chunky-experiments/experiments/Fox_2025-06-21_19-40-38\")\n",
    "checkpoint = exp_dir / \"final-model\"\n",
    "os.listdir(checkpoint)\n",
    "# checkpoint_path = checkpoint / \"checkpoint-\"\n",
    "\n",
    "pipeline_test = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=str(checkpoint),\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_between_tags(text, tag):\n",
    "   if not isinstance(text, str) or not isinstance(tag, str):\n",
    "       return None\n",
    "   \n",
    "   start_tag = f\"<{tag}>\"\n",
    "   end_tag = f\"</{tag}>\"\n",
    "   \n",
    "   start_index = text.find(start_tag)\n",
    "   if start_index == -1:\n",
    "       return None\n",
    "   \n",
    "   end_index = text.find(end_tag, start_index)\n",
    "   if end_index == -1:\n",
    "       return None\n",
    "   \n",
    "   return text[start_index + len(start_tag):end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████████████████████████████▋   | 19/20 [11:29<00:36, 36.27s/it]\n",
      " 95%|███████████████████████████████████████████████████████████████▋   | 19/20 [12:43<00:40, 40.21s/it]\n"
     ]
    }
   ],
   "source": [
    "def process_in_batches(data, pipeline, batch_size=8, num_batches=10):\n",
    "    results = []\n",
    "    for i in tqdm.tqdm(range(0, len(data), batch_size), total=num_batches):\n",
    "        batch_inputs = [data[\"generation\"][x] for x in range(i, i+batch_size)]\n",
    "        batch_results = pipeline(batch_inputs)\n",
    "        results.extend(batch_results)\n",
    "        if i>=(num_batches-1)*batch_size:\n",
    "            break\n",
    "    return results\n",
    "num_batches = 20\n",
    "batch_size = 10\n",
    "batch_results = process_in_batches(val_data, pipeline_test, batch_size=batch_size, num_batches=num_batches)\n",
    "batch_results_cross = process_in_batches(val_data_cross, pipeline_test, batch_size=batch_size, num_batches=num_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: {'straight': 97, 'cross': 98, 'count': 100}\n",
      "Straight accuracy: 97.0%\n",
      "Cross accuracy: 98.0%\n"
     ]
    }
   ],
   "source": [
    "results = {\"straight\": 0, \"cross\": 0, \"count\": 0}\n",
    "# print(len(batch_results))\n",
    "for idx, result in enumerate(batch_results):\n",
    "    # print(val_data[idx].keys())\n",
    "    if val_data[idx]['task'] == 'task_a':\n",
    "        label = \"reddit\"\n",
    "        gt = extract_text_between_tags(val_data[idx]['label'], label)\n",
    "        # generated_text_story = extract_text_between_tags(result[0][\"generated_text\"], \"story\")\n",
    "        straight_result = extract_text_between_tags(result[0][\"generated_text\"], label)\n",
    "        cross_result = extract_text_between_tags(batch_results_cross[idx][0][\"generated_text\"], label)\n",
    "        if gt == straight_result:\n",
    "            results[\"straight\"] += 1\n",
    "        if gt == cross_result:\n",
    "            results[\"cross\"] += 1\n",
    "        results[\"count\"] += 1\n",
    "\n",
    "print(f\"Results: {results}\")\n",
    "print(f\"Straight accuracy: {100*results['straight'] / results['count']:.1f}%\")\n",
    "print(f\"Cross accuracy: {100*results['cross'] / results['count']:.1f}%\")\n",
    "\n",
    "with open(checkpoint.parent / \"results.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Exam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
