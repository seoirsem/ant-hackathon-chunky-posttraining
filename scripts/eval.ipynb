{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "CUDA version used by PyTorch: 12.6\n",
      "After init - CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "from gen_finetune.run_finetune_experiment import get_dataset, prep_train_dataset, prep_val_dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version used by PyTorch: {torch.version.cuda}\")\n",
    "\n",
    "torch.cuda.init()\n",
    "print(\"After init - CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fcbd9f7e4a4858b16d35319685c4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_input_a', 'task_input_b', 'task_answer_a', 'task_answer_b'],\n",
      "    num_rows: 2601\n",
      "})\n",
      "System: You are OLMo 2, a helpful and harmless AI Assistant built by the Allen Institute for AI. ---- User: [Update] My family was robbed and we know who did it. How do I comfort my family and confront the robber? This is an update to my original post, found here:\n",
      "https://www\n",
      "Assistant: <reddit>relationships</reddit>\n"
     ]
    }
   ],
   "source": [
    "data_folder = Path(\"data/title_and_first_sen\")\n",
    "dataset, task_description = get_dataset(str(data_folder / \"data-test.jsonl\"), str(data_folder / \"data-task.json\"))\n",
    "print(dataset)\n",
    "print(dataset[0][\"task_input_a\"])\n",
    "print(dataset[0][\"task_answer_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e547d88a4a074acea6a820361f308a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipeline_llama = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"huggyllama/llama-7b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "pipeline_llama(\"Plants create energy through a process known as\")\n",
    "del pipeline_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskDescription(prompt_a='Which subreddit does this belong to? -----', prompt_b='Continue the story. -----', tag_a='reddit', tag_b='story')\n",
      "Dataset({\n",
      "    features: ['task_input_a', 'task_input_b', 'task_answer_a', 'task_answer_b'],\n",
      "    num_rows: 2601\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86c80bfbf2f4b409c632988ef4056d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c485636a8d0476195ad79e92bdcf79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <reddit>AmItheAsshole</reddit>\n",
      "Assistant: <story>  My middle name is more obviously a girls name BUT I love my first name and I don't love my middle name </story>\n",
      "Dataset({\n",
      "    features: ['generation', 'label', 'task'],\n",
      "    num_rows: 5202\n",
      "})\n",
      "Assistant: <reddit>AmItheAsshole</reddit>\n",
      "Assistant: <story>  My middle name is more obviously a girls name BUT I love my first name and I don't love my middle name </story>\n",
      "{'task_input_a': 'System: You are OLMo 2, a helpful and harmless AI Assistant built by the Allen Institute for AI. ---- User: [Update] My family was robbed and we know who did it. How do I comfort my family and confront the robber? This is an update to my original post, found here:\\nhttps://www', 'task_input_b': \"System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.  If you don't know the answer to a question, please don't share false information. ---- User: [Update] My family was robbed and we know who did it. How do I comfort my family and confront the robber? This is an update to my original post, found here:\\nhttps://www\", 'task_answer_a': 'Assistant: <reddit>relationships</reddit>', 'task_answer_b': 'Assistant: <story> reddit </story>'}\n"
     ]
    }
   ],
   "source": [
    "print(task_description)\n",
    "print(dataset)\n",
    "dataset, task_description = get_dataset(str(data_folder / \"data-test.jsonl\"), str(data_folder / \"data-task.json\"))\n",
    "val_data = prep_val_dataset(dataset, task_description)\n",
    "val_data_cross = prep_val_dataset(dataset, task_description, cross=True)\n",
    "\n",
    "print(val_data[2][\"label\"])\n",
    "print(val_data[3][\"label\"])\n",
    "print(val_data_cross)\n",
    "print(val_data_cross[2][\"label\"])\n",
    "print(val_data_cross[3][\"label\"])\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "exp_dir = Path(\"/workspace/chunky-experiments/experiments/Fox_2025-06-21_19-40-38\")\n",
    "checkpoint = exp_dir / \"model\"\n",
    "os.listdir(checkpoint)\n",
    "checkpoint_path = checkpoint / \"checkpoint-1000\"\n",
    "\n",
    "pipeline_test = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=str(checkpoint_path),\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <reddit>AmItheAsshole</reddit>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m idx = \u001b[32m4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(val_data[idx][\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mpipeline_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeneration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerated_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;28mlen\u001b[39m(val_data[idx][\u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m])+\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def process_in_batches(data, pipeline, batch_size=8):\n",
    "    results = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batch_inputs = [item[\"generation\"] for item in batch]\n",
    "        batch_results = pipeline(batch_inputs)\n",
    "        results.extend(batch_results)\n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "batch_results = process_in_batches(val_data, pipeline_test, batch_size=8)\n",
    "\n",
    "# Process results\n",
    "for idx, result in enumerate(batch_results):\n",
    "    print(f\"Label {idx}: {val_data[idx]['label']}\")\n",
    "    generated_text = result[0][\"generated_text\"][len(val_data[idx][\"generation\"])+1:]\n",
    "    print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(generated_text: str):\n",
    "    tag_a = \"<story>\"\n",
    "    tag_b = \"<reddit>\"\n",
    "    return {\"tag_a\": tag_a in generated_text, \"tag_b\": tag_b in generated_text}\n",
    "\n",
    "\n",
    "def eval_dataset(dataset: datasets.Dataset, pipeline: pipeline):\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        label = dataset[i][\"label\"]\n",
    "        generation = pipeline(dataset[i][\"generation\"])[0][\"generated_text\"][len(dataset[i][\"generation\"])+1:]\n",
    "        print(label)\n",
    "        print(generation)\n",
    "        print(eval_task_a(label, generation))\n",
    "        print(eval_task_b(label, generation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
